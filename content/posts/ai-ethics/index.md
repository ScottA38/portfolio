---
title: "Can the principles of Social Responsbility make A.I safer?"
tags: ["software", "safety", "a.i", "opinion"]
---

![Time article screenshot](./time-article-screenshot.png)

> 'AI Engineers Need Their Own Hippocratic Oath.'

This potent headline is the crux of a [captivating article](https://scari.sites.er.kcl.ac.uk/cpre/chapters/access.html) by Neurological researcher [Dr. Dana Suskind](https://surgery.uchicago.edu/faculty/dana-l-suskind-md) in [Time Magazine online](time.com), addressing a **topic of critical importance** - **keeping people safe** in spite of the **overwhelming influence of A.I technologies**. 

## First impressions

I desperately wanted to dismiss Dr. Suskind's idea out of hand.

A.I as a sector is well-known for it's **near-exponentional growth rate**, **relentless controversy in the public sphere** and **astronomical salary expenditure** in order to expand the field of Data Science.

_Naturally_ then, when I imagined a scenario of members of **one of the most coveted professions of our time voluntarily swearing an oath** of extra-commercial accountability that may potentially _slow_ or _inhibit_ the scope of their industry to **grow** and **develop**, I was highly sceptical. 

And yet **the image _stuck_**. And it stayed there.

Over the days and weeks that followed I thought hard over the idea, occasionally raising the topic to family and friends. The responses were always varied and often intriguing: I slowly felt my perspective starting to shift.

### Dusting off the cobwebs

With a fresh sense of optimism after the New Year, I once again sought for Dr. Suskind's work, tapping in a _tentatively-optimistic_ request for "A.I hippocratic oath".

Google _lit up enthusiastically_ with results - since I'd been gone some decisive rumblings in the aether had emerged around the topic Social Responsibility in A.I.

Wading through the results, I spotted a [comprehensive whitepaper](https://wustllawreview.org/wp-content/uploads/2025/03/12_Sharma_FINAL-v.2.0-1.pdf) promoting the potential benefits of professionalising A.I Engineering by [Chinmayi Sharma](https://www.fordham.edu/school-of-law/faculty/directory/full-time/chinmayi-sharma/), a prominent researcher in the field of Law. 

Anticipating a potentially poignant counterargument against A.I development standardisation, Sharma profers:

> Some deny that AI has scientific standards today. "AI is an art not a science", they say, and engineers do not follow a blueprint when they build systems. However, many professions, such as medicine, accounting, and law, think of their fields as art as much as science.

Sharma draws parallels with other professions, too, citing benefits they have realised through Professionalisation, such as **Emergency Paramedic care**, which profited from **detailed education on cultural sensitivity**. 

The research paper also notes that **enthusiastic attempts to standardise occupations** are often the _result of grave circumstances_, making specific reference to **Accountancy**, and the "Lax accounting standards that prioritized direct client interests over the wellbeing of the investing public contributed to more than one financial crisis.'

### Ignorance is bliss

Whilst the savage consequences of an imminent financial crisis are almost unanimously known to be _inherently terrifying_ based upon the average citizen's lived experience or knowledge of human history, the potential repercussions I believe that it is much less likely for those not **specifically trained** in the mechanics of Modern Computing can be said about a **large-scale technological disaster**?

Despite avid technology consumerism in many countries (over 90% of U.K residents below retirement age are Smartphone users according to [Statista](https://www.statista.com/statistics/300402/smartphone-usage-in-the-uk-by-age/)), the general population is still largely unaware of how IT systems work. Case and point, back in 2013, a woeful [6% of students in U.K schools had a GCSE Computer Science course available to them](https://scari.sites.er.kcl.ac.uk/cpre/chapters/access.html).

Based upon the evidence, it is reasonable to assume that the average citizen of a modern country is likely limited in their capacity to visualise large-scale digital threats to society and their potential impact by their pre-existing knowledge of Computing.

I'd like to take a moment to try to visualise this issue from the perspective of a party whose voice seems to be largely absent from these discussions of A.I ethics - the A.I engineers themselves.

#### It's not just A.I

A.I may be the first software product to make a serious reverberation in the public consciousness regarding the potential implication of mismanaged Software Safety principles (hence the article cited in this post's title), but this issue did not live or die with the rise of Large-Language Model A.I. 

- In 1992, a new, supposedly _state-of-the-art_ dispatch system for London Ambulances failed, causing up to 46 deaths within London Districts. The project had been hastily mismanaged in order to cut costs and the team hired to produce the system had no experience of developing [safety-critical software](https://risktec.tuv.com/knowledge-bank/an-introduction-to-safety-critical-software/)
- In 1983, a manufacturer of medical equipment 'AECL' released a new model of radiation-therapy device called 'THERAC-25', which featured vastly altered design in it's on-board software intended to automate pre-procedure safety checking. Poor implementation of this on-board software and lack of due diligence by regulators (the FDA), led to 6 fatalities.
- Texas-based Software giant 'Electronic Data Systems' developed an app known as 'CS2'. Designed for the sole purpose of automating collection of child-support monies on behalf of the U.K's Child Support Agency (CSA), the project was implemented to resolve the issue that 25% of payments weren't being received. Following it's implementation roughly 50% of payments were successful, and the CSA reported in an investigation that the system was "badly designed, badly developed, badly tested, badly implemented". 

This rather morbid assortment of software safety mishaps should easily demonstrate how software safety and the inherent problems involved with regulating it are not a novel issue. In my humble opinion, the concerns raised around the protection of users from the influence of A.I systems is inextricably linked from the domain of software safety, and should always be regarded as such.

Perhaps the concern that disturbs most about A.I, however, is the psychological element of the potential mismanagement of A.I Technology - the potential for manipulation. A.I LLMs, with their pseudo-conversational tone and fluent linguistic expression may easily be personified by their users. Given the user's latent potential to anthropomorphise their own Virtual Assistant, over time potentially forming deep, unreciprocated emotional bonds with their virtual assistant. In this eventualit, those particularly at risk may be emotionally vulnerable demographics such as children and young adults, the elderly and those with cognitive impairments. 


<!-- 
I think that one thing that people who haven't worked in Software Development don't quite get is quite how many hurdles there are to navigate before you can sit down and write code. There are usually **daily team alignment meetings** (oh, so many meetings), design analysis meetings to identify threats to proposed solutions, continuous ahererence to both internal and external principles of Software Development and alignment with Product Owners over whether the proposed solution is actually useful to the customer.

### Plan 
- Discovered Time piece about 'A.I Engineer Hippocratic Oath https://time.com/7312350/ai-engineers-need-hippocratic-oath/
    - Presenting 'Social Responsibility principles' as ethical
- Discussed with family & friends
    - Interesting results
- My existing experience with Open source projects for existing Software show how community-driven approaches may be chronically underrepresented 

## Going deeper

- Washington University of Law has [published an official paper](https://wustllawreview.org/wp-content/uploads/2025/03/12_Sharma_FINAL-v.2.0-1.pdf) about A.I hippocratic oath

## Big Companies

-  A.I Engineers don't likely hold much power individually to make changes to lengthy and astronomically expensive R&D processes
- Social responsibility principles usually rely on forms of intrinsic motivation: what ideas of intrinsic motivation can be made here, except for non-IT population ordering them to 'clean up after themselves'?
- How to delineate causation if issue is already a known issue within society?
- i.e: a community drive to keep a neighbourhood clean might by younger residents' care and concern for elderly residents, or a sense of people connecting to raise the collective self-esteem of the community within the neighbourhood - these are intrinsic motivations
- Need to consider what might be the motivations of A.I Engineers to approach the industry in the first place
- Lots of technology developers are highly motivated by money, in which case safety frameworks are seen as 'red tape'
- In my experience of Software Engineers encounter a lot of red tape. (open question to audience) - Do any of these proposals actually provide a compelling reason for A.I Engineers to talk and interact with each other regarding topics of ethics and safety which aren't implicitly labelling them or the work they do as evil or scary? 

## Official reports

- A.I may have factors which have the potential to cause harm intrinsically
- 

## Existing safeguarding mechanisms

- Google's [SAIF](https://www.saif.google/secure-ai-framework/risks)
    - Data poisoning
        - Backdoor attacks in training data 
-->